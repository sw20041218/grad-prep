# Optimization Trick 1: ReLU vs tanh（为什么 ReLU 更好训）

## 目标

通过可视化，直观看到：
- tanh 网络的训练特性
- ReLU 网络的训练特性
并理解：为什么 ReLU 在深层网络中更常用。

---

## 实验设计

- 同一个任务：拟合 y = sin(x)
- 同一个网络结构：1 hidden layer
- 唯一不同：激活函数（tanh vs ReLU）
- 对比指标：
  - loss 曲线
  - 收敛速度

---

## 关键直觉

tanh：
- 有饱和区
- 大输入时梯度接近 0

ReLU：
- 正区间梯度恒为 1
- 梯度传播更顺畅

---

## 观察重点

1. 哪个下降更快？
2. 哪个更稳定？
3. 哪个更容易“卡住”？