# Linear Regression（线性回归）— 从数学到模型的第一步

## 1. 为什么线性回归是“模型学习的起点”

在线性代数和优化中，我们已经学过：

- 函数可以被最小化
- 梯度下降可以找到最小值

线性回归做的事情非常直接：

> 用一个最简单的模型，
> 去拟合一组数据，
> 并用梯度下降来训练它。

---

## 2. 模型是什么？

线性回归模型是一条直线：

y = w x + b

其中：
- w：斜率（权重）
- b：截距（偏置）

这两个量就是 **模型参数**。

---

## 3. 预测在干什么？

给定一个 x，模型输出：

y_pred = w x + b

这只是一次普通的函数计算。

---

## 4. loss 是什么？

loss 用来衡量模型“错得有多离谱”。

最常见的是平方误差：

loss = (y_pred - y_true)^2

注意：
- loss 是一个 **关于参数 (w, b) 的函数**
- 训练的目标是：让 loss 变小

---

## 5. 训练在干什么？

训练 = 优化问题：

> 在参数空间 (w, b) 中，
> 对 loss 函数做梯度下降。

也就是说：
- 梯度不是对 x 求
- 而是对 **参数 w, b 求**

---

## 6. 一个关键认知（非常重要）

> 梯度下降不是在“数据空间”里走，
> 而是在“参数空间”里走。

这是理解所有机器学习模型的核心。

---

## 7. 本阶段的完成标准（封账）

当你能自然说出这句话：

> “训练线性回归模型，
> 本质是在参数 (w, b) 空间中，
> 对平方损失函数做梯度下降。”

这一块就已经完成。