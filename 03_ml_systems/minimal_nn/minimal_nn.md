# Minimal Neural Network（最小神经网络）— 从线性回归到非线性

## 1. 为什么需要神经网络？

线性回归：y = w^T x + b
它只能表达“线性关系”。

如果真实关系是弯的（例如 y = sin(x)、抛物线、分段函数），
线性模型会天然拟合不动。

神经网络的最小升级就是：
> 在线性变换后，加一个非线性函数（激活函数）。

---

## 2. 本次用的网络结构（1 隐藏层）

输入 x（1维） → 隐藏层（H个神经元） → 输出 y_pred（1维）

记号（向量化写法）：
- W1: (H, 1), b1: (H,)
- W2: (1, H), b2: (1,)

前向传播（forward）：
1) z1 = X @ W1^T + b1        # (N, H)
2) a1 = tanh(z1)             # (N, H)  非线性在这里
3) y_pred = a1 @ W2^T + b2   # (N, 1)

---

## 3. loss（仍然用 MSE）

loss = mean((y_pred - y_true)^2)

注意：训练仍然是“最小化 loss”，和线性回归完全一致。

---

## 4. 训练为什么变难了？

区别只在一件事：
- 线性回归：loss 对参数的梯度很直接
- 神经网络：中间多了一层 tanh（非线性），必须用链式法则把梯度“传回去”
这就是反向传播（backprop）。

---

## 5. 本阶段封账标准

你能说清楚这三句话就算完成：

1) 神经网络 = 线性层 + 激活函数 + 线性层
2) 训练仍然是梯度下降，只是梯度用链式法则算出来
3) 非线性让模型能拟合“弯曲关系”，这是线性回归做不到的