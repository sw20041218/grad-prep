# Xavier / He Initialization（干中学）

## 目的
通过一个极小的神经网络实验，观察：
- 不同初始化 scale
- Xavier 初始化
- He 初始化

对训练 loss 的影响。

我们不推公式，只看现象。

---

## 实验设置
- 任务：拟合 y = sin(x)
- 网络：1 个隐藏层（非常浅）
- 激活函数：tanh / ReLU
- 优化：SGD
- 对比不同初始化方式

---

## 初始化方式

### 1. 固定 scale 初始化
- scale = 0.01
- scale = 0.1
- scale = 1.0
- scale = 3.0

### 2. Xavier 初始化
- 适合 tanh

### 3. He 初始化
- 适合 ReLU

---

## 观察重点（你要盯着看）
1. 哪些曲线一开始就炸？
2. 哪些下降很慢？
3. 哪些又快又稳？
4. tanh 和 ReLU 在不同初始化下的区别

---

## 结论（跑完后自己补）
- Xavier 更适合：_____
- He 更适合：_____
- scale 过大时的现象：_____
- scale 过小时的现象：_____