# PCA（主成分分析）｜从“找方向”到“压缩信息”

目标：把你刚学的特征值/特征向量用在真实问题上——
**给一堆数据点，找到“最重要的方向”，并把数据投影到这些方向上，实现降维。**

---

## 1. PCA 在解决什么问题？

当数据维度很高时：
- 可视化困难
- 噪声多
- 计算/学习成本高

PCA 想做的是：
- 找到一个新的坐标系（新的轴）
- 第一根轴：让数据在这根轴上的“变化最大”（信息最多）
- 第二根轴：在与第一根轴正交的前提下变化最大
- ...

然后把数据投影到前 k 根轴上，得到低维表示。

---

## 2. 为什么要中心化（减均值）？

PCA 关心的是o的是“数据的变化（方差）”，而不是“数据离原点有多远”。

中心化做法：
- 对每一维减去该维均值
- 让数据云以 0 为中心

不中心化会导致：
- 方向被“均值偏移”污染，主方向不代表真实变化。

---

## 3. 协方差矩阵是什么？

假设中心化后的数据矩阵为 X，形状为 (n_samples, n_features)。

协方差矩阵常写为：

C = (1/(n-1)) * X^T X

直觉：
- C 描述不同维度之间“共同变化”的程度
- 对角线：每一维自己的方差
- 非对角线：维度之间的相关性

---

## 4. PCA 为什么用特征分解？

关键事实：

- C 的特征向量：表示“稳定方向”（在这个二次型意义下）
- C 的特征值：表示该方向上的方差大小

因此：
- 最大特征值对应的特征向量 = 数据变化最大的方向 = 第一主成分
- 第二大特征值对应的特征向量 = 第二主成分
- ...

---

## 5. PCA 输出是什么？

PCA 最终给你三样东西：
1) 主成分方向矩阵 W（列向量是主成分）
2) 解释方差比（每个主成分解释了多少信息）
3) 低维表示 Z = X_centered @ W_k

---

## 6. 学完本项目你应该能回答

1) 为什么一定要中心化？
2) 为什么主成分是协方差矩阵的特征向量？
3) 为什么最大特征值代表“信息最多”？
4) 投影 Z 的形状是什么？含义是什么？

---
## 7. 小结（跑完 .py 再补）

PCA = 找方向（主成分） + 做投影（降维）。
特征向量告诉你方向，特征值告诉你每个方向有多重要。
