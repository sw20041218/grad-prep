# BatchNorm：初始化是否还重要？

## 实验目的
观察在 **非常差的初始化** 下：
- 不使用 BatchNorm
- 使用 BatchNorm

训练曲线有什么本质区别。

---

## 实验设置
- 任务：拟合 y = sin(x)
- 网络：1 隐藏层 ReLU
- 初始化：scale = 3.0（故意设得很糟）
- 优化：SGD
- 对比：是否加入 BatchNorm

---

## 观察重点
1. 不加 BN 时，loss 是否震荡 / 停滞？
2. 加 BN 后，是否能稳定下降？
3. 初始化是否“没那么重要”了？

---

## 结论（跑完后填写）
- 没有 BN 时的现象：
- 加 BN 后的变化：
- BN 在做的事情（用一句话）：